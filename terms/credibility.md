The concept of credibility is essentially a *heuristic*, or a practical shortcut, by which [agents](https://github.com/gcassel/Modular-Organization-Terminology/blob/master/terms/agent.md) extend **credit** to [descriptions](https://github.com/gcassel/Modular-Organization-Terminology/blob/master/terms/description.md) of *experiences* or *concepts*.  
 
Assigning credibility is heuristic when we’re just *saving time* by not *inspecting* [information](https://github.com/gcassel/Modular-Organization-Terminology/blob/master/terms/information.md) personally, and it’s *also* heuristic when the supposed information-- a descriptive *indication*, *declaration* or *claim*-- is not directly available. For instance, few people will ever run a particle accelerator, or read its results, but we may extend credibility to scientists who use them.  

Another example:  Americans are quite likely to believe that Indonesia exists, despite the fact that few of them travel there, because of an immensely interlaced [network](https://github.com/gcassel/Modular-Organization-Terminology/blob/master/terms/network.md) of consistent claims about that part of Earth.
 
Credibility ultimately derives most of its validity from [peers](https://github.com/gcassel/Modular-Organization-Terminology/blob/master/terms/peer.md) who can inspect and fact-check claims. This accountability can create a limited but powerful “honesty of crowds”. 

Accountability generally depends upon *retroactive responsibility*. For instance, most people won’t lie, nor guess wildly and irresponsibly, if they expect their claims to be thoroughly inspected and fact-checked later.
 
We develop **trust** in agents who repeatedly pass inspection and fact-checking– whether by us personally, by their peers, or even by random strangers– for a simple and fundamentally important reason: a good **reputation** can only be *developed* through significant time and effort, and can be quickly *damaged* or *destroyed* by a single incident of severe *misinformation* or *disinformation*.
